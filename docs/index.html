<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="google-site-verification" content="JWOue1ZxNWRUMycXffn9ST4zeYFgqa01tDaUz4tDkAY" />
  <title>S2R-ViT</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>
<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <div class="title" style="padding-top: 25pt;">  <!-- Set padding as 10 if title is with two lines. -->
      <span style="color: rgb(35, 120, 155); font-weight: bold; font-style: italic;">S2R-ViT</span>&nbsp for multi-agent cooperative perception: Bridging the gap from simulation to reality
    </div>
    

  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <a href="https://jinlong17.github.io/" target="_blank">Jinlong Li</a><sup>1</sup>,&nbsp
    <a href="https://derrickxunu.github.io/" target="_blank">Runsheng Xu</a><sup>2</sup>,&nbsp
    <a href="https://scholar.google.com/citations?user=fGK5P7IAAAAJ&hl=zh-CN" target="_blank">Xinyu Liu</a><sup>1</sup>,&nbsp
    <a href="https://scholar.google.com/citations?user=d94_GW4AAAAJ&hl=en" target="_blank">Baolu Li</a><sup>1</sup>,&nbsp
    <a href="https://scholar.google.com/citations?user=dJ8izFAAAAAJ&hl=en" target="_blank">Qin Zou</a><sup>3</sup>,&nbsp
    <a href="https://mobility-lab.seas.ucla.edu/" target="_blank">Jiaqi Ma</a><sup>2</sup>,&nbsp
    <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en" target="_blank">Hongkai Yu</a><sup>1*</sup>,&nbsp
  </div>
  <div class="institution">
     <sup>1</sup> Cleveland State University,
     <sup>2</sup> UCLA,
     <sup>3</sup> Wuhan University
  </div>


  <div class="link">
    <a href="https://arxiv.org/pdf/2307.07935" target="_blank">[Paper]</a>&nbsp;
    <a href="https://github.com/jinlong17/S2R-ViT" target="_blank">[Github Code]</a>
  </div>
  <div class="teaser">
    <img src="assets/V2V_gap.png"  style="width: 70%; height: auto;">
  </div>

 <div class="body">
Our research is focused on utilizing labeled   simulated data and unlabeled real-world data as transfer   learning to reduce the domain gap for multi-agent cooperative perception. Illustration of the <span style="color: rgb(35, 120, 155); font-weight: bold;"> Domain gap (Deployment Gap, Feature Gap)</span>  for multi-agent cooperative perception from simulation to reality. Here we use Vehicle-to-Vehicle (V2V) cooperative perception in autonomous driving as example. CAV indicates the Connected Autonomous Vehicles.

 </div>


</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Overview</div>
  <div class="body">
    This paper is the first work that investigates the domain  gap on multi-agent cooperation perception from simulation to reality, specifically focusing on the deployment gap and  feature gap in point cloud-based 3D object detection. Based on the analysis, we present  <span style="color: rgb(35, 120, 155); font-weight: bold;"> the first Simulation-to-Reality transfer learning framework using a novel Vision Transformer, named S2R-ViT</span>, to mitigate these two types of domain gaps, which mainly contain an Uncertainty-aware Vision Transformer and an Agent-based Feature Adaptation module. The experiment shows the effectiveness of S2R-ViT.   This research presents a significant step forward in the multiagent cooperation perception from simulation to reality.

    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="assets/S2R-ViT.png" width="95%"></td>
      </tr>
    </table>


  </div>
</div>
<!-- === Overview Section Ends === -->


<!-- === Section Starts === -->
<div class="section">
  <div class="title">S2R-UViT: Simulation-to-Reality Uncertainty-aware Vision Transformer</div>

    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="assets/S2R-UViT.png" width="95%"></td>
      </tr>
    </table>


    <div class="body">

      The Deployment Gap from simulation to reality brings  different uncertainties to both ego and neighboring agents, e.g., spatial bias by GPS errors, spatial misalignment in  the coordinate projection because of communication latency. <span style="color: rgb(35, 120, 155); font-weight: bold; font-style: italic;">How to effectively reduce the degradation effects of these uncertainty drawbacks is an essential problem and open question to the S2R multi-agent perception research</span>. In this paper, we propose to answer this question from two  perspectives: uncertainties can be relieved by enhancing <span style="font-weight: bold;"> (1) the feature interactions across all agents' spatial positions more comprehensively and (2) the ego-agent features by considering the shared other-agent features of different uncertainty levels. These two perspectives motivate us to develop the novel Local-and-Global Multi-head Self Attention   (LG-MSA) Module and Uncertainty-Aware Module (UAM) respectively.</span>


  </div>

</div>
<!-- === Section Ends === -->




<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Qualitative Results</div>
  <div class="body">

    Robustness in Deployment-Gap Scenario.
    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="assets/Robustness.png" width="100%"></td>
      </tr>
    </table>

  </div>
</div>

<!-- === Result Section Ends === -->





<!-- === Reference Section Starts === -->
<div class="section">
  <div class="bibtex">BibTeX</div>
<pre>
  @inproceedings{li2024s2r,
    title={S2r-vit for multi-agent cooperative perception: Bridging the gap from simulation to reality},
    author={Li, Jinlong and Xu, Runsheng and Liu, Xinyu and Li, Baolu and Zou, Qin and Ma, Jiaqi and Yu, Hongkai},
    booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
    pages={16374--16380},
    year={2024},
    organization={IEEE}
  }
</pre>

  </div>
</div>
<!-- === Reference Section Ends === -->


</body>
</html>
